This section will cover the different approaches for automation based on machine learning algorithms and will take a closer look at large language models used by thousands of software developers today. Additionally we will look at supervised machine learning algorithms that have been experimented with and create a comparison between them. Lastly, we will be taking a look at dedicated models with the example of DNNFFz.
\subsection{Large Language Models based on GPT Models}
This section covers the experiments' results of the paper \cite[AI-Driven Refactoring: A Pipeline for Identifying and Correcting Data Clumps in Git Repositories]{baumgartner2024aidriven}.\\
In the fields of ai assisted workflows, large language models (LLMs) have shifted from a niece speciality to an almost omnipotent tool which finds applications from image creation to code generation.\cite{meyer2024ai} LLMs are huge deep-learning models pre-trained on enormous amounts of data. They are especially known for their ability to be trained on datatest of specific domains but can also be used on a broad spectrum of general knowledge, making them incredibly flexible. \cite{baumgartner2024aidriven}
Being the best-known LLM, OpenAI's Generative Pre-trained Transformer series (GPT) with its versions GPT-3.5, GPT-3.5 Turbo and GPT-4.
Temperature is a key parameter for GPT models, having a value ranging from zero to one, this parameter determines the predictability of the results. 
A higher temperature leads to more variety in the LLM and vice versa.
\subsubsection{Detection}
Taking a look at detection itself, the median sensitivity of GPT-3.5-Turbo is 0 which indicates many data clumps are undetected and many false positives.
Submitting all files in bulk also made the model trade off its sensitivity with the specificity parameters, with the median sensitivity reaching 50 percent, but the specificity only being 14 percent. 
Apparently, the model is looking at all the information and is finding more data clumps. But also potentially leading to more false positives.
The temperature also has a similar trade-off. Higher temperatures lead to a lower sensitivity and the other way around.
\subsubsection{Refactoring}
If you prompt a GPT model to refactor source code while also giving it the location of the data clumps, GPT-3.5 and GPT-4's median is identical, lying at 68 percent.
These results show if you know where to look for data clumps and which places to refactor, both models can refactor the source code just as well as the other.
GPT-3.5-Turbos arithmetic mean is less which can be explained by the existence of more overall compiler errors.
On the same note: the median of the three instruction variants is also identical.
Higher temperature values also resulted in a median of 0 percent, indicating more non-compilable code.
\subsubsection{Combining Detection and Refactoring}
The final step of the experiment is combining detection and refactoring into one step. At this point, the limitations of GPT-3.5-Turbo become clear. The model scores a median score of 7 percent compared to 82 percent of GPT-4.
Surprisingly, however, providing no definitions about data clumps leads to the best results, reaching a median of 46 percent. 
All other instruction types are 0 percent each.
Another experiment, held in the paper mentioned in section \ref{introduction}, \cite[The effectiveness of supervised machine learning algorithms in predicting software refactoring]{aniche2020effectiveness}, compared different machine learning models with each other, with the "Random Forest" Model performing best out of all the tested models.\\
It appears as if machine learning models commonly perform best in a close-to-random environment.
\subsection{Comparing Linear Regression, linear SVM, Naive Bayes, Decision Trees, Random Forest, and Neural Network}
This section covers the experiments' results of the article \cite[The effectiveness of supervised machine learning algorithms in predicting software refactoring]{aniche2020effectiveness}.
\subsubsection{Overview of the used algorithms}
First we will briefly go over the differences between these algorithms, starting with Logistic Regression.
\begin{itemize}
    \item Logistic Regression (LR) \cite{biship2007pattern} is centered on combining input values using coefficient values to predict an outcome value.
    \item (Gaussian) Naive Bayes algorithms \cite{zhang2014optimality} use training data to compute the probability of each outcome based on the information extracted from the feature values.
    \item Support Vector Machines (SVMs) \cite{cortes1995support} search for the best hyper-plane to separate the training instances into their respective classes in high-dimensional space.
    \item Decision Trees \cite{quinlan2014c4} yield hierarchical models composed of decision nodes and leaves, presenting a partition of the feature space.
    \item Random Forest \cite{breiman2001random} is an algorithm using a number of decision trees with random subsets of the training data.
    \item Neural Networks \cite{goodfellow2016deep} create an architecture that is similar to neurons and are made up of one or more layers of these neurons. They essentially act as a function, mapping inputs to their respective classes.
\end{itemize}
In the conducted experiment, a pipeline for each of the previously mentioned algorithms validates the outcomes and returns the precision, recall, and accuracy of all the models. Once a classification model is trained for a given refactoring, the model would predict true in case an element (i.e. a class, method, or a variable) should undergo a refactoring or false if it should not.
\subsubsection{Research Questions}
The experiment aims to answer three research questions (RQs): 
\begin{itemize}
    \item[RQ1] How Accurate are Supervised ML Algorithms in Predicting Software Refactoring? This question explores how accurately the different models predict refactoring opportunities, while using Logistic Regression as a base line.
    \item[RQ2] What are the Important Features in the Refactoring Prediction Models? This question regards features that are most relevant to the models and have the biggest impact on the outcome.
    \item[RQ3] Can the Predictive Models be Carried Over to Different Contexts? Whether or not refactoring prediction models \textit{need} to be trained for this specific context or a more generalized model is sufficient potentially reduces the cost of applying and re-training the models in the real-world. The question is tackled by comparing the accuracy of predictive models against independent datasets.
\end{itemize}
\subsubsection{Data Sources}
The targeted projects for the experiment are collected from three different sources: The Apache Software Foundation (ASF), F-Droid (a software repository of Android mobile apps), and GitHub. They also used a highly sophisticated method to extract the labelled instances using RefactoringMiner\cite{tsantalis2018accurate}, a tool for refactoring detection having an average precision of 99.6 percent\cite{tsantalis2022refactoringminer}.
\subsubsection{Answering the Research questions}
Evaluation gave answers to the three research questions formulated above. 
\begin{itemize}
    \item[RQ1] Observation 1: \textit{Random Forest models are the most accurate in predicting software refactoring.} It's average accuracy for class, method, and variable-level refactorings are 0.93, 0.90, and 0.94 respectively. The second best model is Decision Trees which only falls shortly behind with a score .04 less on almost every dataset compared to Random Forest.\\Observation 2: \textit{Random Forest was outperformed only a few times by Neural Networks.} Specifically it outperformed Random Forest 4 times in terms of accuracy in the F-Droid dataset, and in two opportunitiesin both the Apache and GitHub databases, with the difference always lying at around 1 percent. \\ Observation 3: \textit{Naive Bayes models present high recall, but low precision.} They presented recalls of 0.94, 0.93, 0.94, and 0.84 in the combined dataset. Unfortunately, these models had by far the worst precision values: 0.62, 0.66, 0.62, and 0.67 in the same datasets. \\ Observation 4: \textit{Logistic Regression shows good accuracy.} The most straightforward model in the study presented a fairly high average accuracy, consistently outperforming Naive Bayes models with an average of 0.83 across all datasets. 
    \item[RQ2] Observation 5: \textit{Process metrics are highly important in class-level refactorings.} The top ranking metrics are quantity of commits, lines added in a commit, and number of previous refactorings. Top-5 to Top-10 rankings mostly consist of process metrics with the occational ownership metrics, appearing 32 times in the top-1 ranking. \\ Observation 6: \textit{Class-level features play an important role in method-level and variable-level refactorings.} In the top-1 ranking for method-level refactoring models, 13 out of 17 features are class-level features. Variable-level shows the same for 11 of the features. \\ Observation 7: \textit{Some features never appear in any of the rankings.} Number of default values, and the number of synchronized fields never appear in the top-10 ranking. Nine other features never appear in the top-10 ranking, and ten features never make it in the variable-level refactoring models.
    \item[RQ3] Observation 8: \textit{Random Forest still presents excellent precision and recall when generalized, but smaller when compared to previous results.} Training Random Forest models with the GitHub dataset and testing it in Apache achieves a precision of 0.87 and recall of 0.84 and still performs reasonably well when trained on smaller datasets. However, Random Forest performs remarkably better when trained on the specific datasets.\\Observation 9: \textit{Method and variable-level refactoring models perform worse than class-level refactoring.} Using Random Forest models trained on the GitHub dataset and testing on the F-Droid dataset, the average precision and recall on class-level is 0.92. In contrary, average precision and recall at method-level are 0.77 and 0,72 respectively; at variable-level, 0.81 and 0.75 for precision and recalls are observed.\\ Observation 10: \textit{SVM outperforms Decision Trees when generalized.} RQ1 revealed Decision Trees being the second best model. In RQ3, however, SVM is now the second best with trumping Decision Trees with an average difference of 0.02 between both models.\\ Observation 11: \textit{Logistic Regression is still a somewhat good baseline.} Trained on the GitHub dataset and tested on the Apache dataset, it shows an average precision and recall of 0.84 and 0.83, with the worst results when trained on the Apache dataset and tested on F-Droid. \\ Observation 12: \textit{Heterogeneous datasets might generalize better.} The apache and F-Droid datasets present lower precision and recall when carried to other contexts. Cross testing these datasets never went beyond precision and recall values beyond 0.78, which was not the case for GitHub, which is a more heterogeneous dataset.
\end{itemize}
\subsubsection{Summary}
The experiemtns main findings show that Random Forest models outperform other Machine Learning models in predicting software refactoring in almost all tests. Furthermore, process and ownership metrics seem to play a crucial role in the creation of beyyer models. Finally, models trained with data from heterogeneous projects generalize better and achieve good performance.\\
More importantly: ML algorithms can be used to accurately model the refactoring recommendation problem.