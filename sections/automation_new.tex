This section will cover the different approaches to automation based on machine learning algorithms. We will also take a closer look at large language models used by thousands of software developers today. Additionally, we will compare supervised machine learning algorithms that have been experimented with.
\subsection{Large Language Models based on GPT Models}
This section covers the experiments' results of the paper \cite[AI-Driven Refactoring: A Pipeline for Identifying and Correcting Data Clumps in Git Repositories]{baumgartner2024aidriven}.\\
In the fields of ai assisted workflows, large language models (LLMs) have shifted from a niche speciality to an almost omnipotent tool which finds applications from image creation to code generation \cite{meyer2024ai}. LLMs are huge deep-learning models pre-trained on enormous amounts of data. They are primarily known for their ability to be trained on the data of specific domains but can also be used on a broad spectrum of general knowledge, making them incredibly flexible \cite{baumgartner2024aidriven}.
Being the best-known LLM, OpenAI's Generative Pre-trained Transformer series (GPT) with its versions GPT-3.5, GPT-3.5-Turbo and GPT-4.\\
Temperature is a crucial parameter for GPT models, having a value ranging from zero to one. This parameter determines the predictability of the results.
A higher temperature leads to more variety in the LLM and vice versa.
\subsubsection{Detection}
Looking at the detection process itself, the median sensitivity of GPT-3.5-Turbo is 0, which indicates that many data clumps are undetected and that there are many false positives.
Submitting all files in bulk also caused the model to trade off its sensitivity with the specificity parameters, with the median sensitivity reaching 50 per cent but the specificity only being 14 per cent.
The model is looking at all the information and finding more data clumps, which could potentially lead to more false positives.
The temperature also has a similar trade-off. Higher temperatures lead to a lower sensitivity and the other way around.
\subsubsection{Refactoring}
If a software engineer prompts a GPT model to refactor source code while also giving it the location of the data clumps, GPT-3.5 and GPT-4's median is identical, at 68 per cent.
These results show that if one knows where to look for data clumps and which places to refactor, both models can refactor the source code and the other.
GPT-3.5-Turbos arithmetic mean is less, which can be explained by more overall compiler errors.
On the same note, the median of the three instruction variants is identical.
Higher temperature values also resulted in a median of 0 per cent, indicating more non-compilable code.
\subsubsection{Combining Detection and Refactoring}
The final step of the experiment is combining detection and refactoring into one step. At this point, the limitations of GPT-3.5-Turbo become clear. The model scores a median score of 7 per cent compared to 82 per cent of GPT-4.
Surprisingly, however, providing no definitions about data clumps leads to the best results, reaching a median of 46 per cent.
All other instruction types are 0 per cent each.
The experiment covered in the next subsection, \ref{comparingAlgorithms}, will compare different machine learning models with each other, with the "Random Forest" Model performing best out of all the tested models.\\
Machine learning models commonly perform best in a close-to-random environment.
\subsection{Comparing Logistic Regression, linear SVM, Naive Bayes, Decision Trees, Random Forest, and Neural Network} \label{comparingAlgorithms}
This section covers the experiments' results of the article \cite[The effectiveness of supervised machine learning algorithms in predicting software refactoring]{aniche2020effectiveness}.
\subsubsection{Overview of the used algorithms}
First, we will briefly review the differences between these algorithms, starting with Logistic Regression.
\begin{itemize}
    \item Logistic Regression (LR) \cite{biship2007pattern} involves predicting an outcome by combining input features using weighted coefficients.
    \item (Gaussian) Naive Bayes classifiers \cite{zhang2014optimality} determine the likelihood of each outcome by analyzing the probabilities derived from the feature values in the training data.
    \item Support Vector Machines (SVMs) \cite{cortes1995support} identify the optimal hyper-plane in high-dimensional space that distinctly separates the training data into different classes.
    \item Decision Trees \cite{quinlan2014c4} create hierarchical models with decision nodes and leaves, effectively partitioning the feature space.
    \item Random Forest \cite{breiman2001random} generates multiple decision trees by using random subsets of the training data, resulting in a robust classification model.
    \item Neural Networks \cite{goodfellow2016deep} create an architecture similar to neurons and are made up of one or more layers of these neurons. They essentially act as a function, mapping inputs to their respective classes.
\end{itemize}

In the experiment, a pipeline for each of the previously mentioned algorithms validates the outcomes and provides the accuracy, recall, and precision metrics for all the models. After training a classification model for a specific refactoring, the model will predict 'true' if an element (such as a class, method, or variable) requires refactoring and 'false' otherwise.
\subsubsection{Research Questions}
The experiment aims to answer three research questions (RQs):
\begin{itemize}
    \item[RQ1] \textit{Are Supervised ML Algorithms accurate in Predicting Software Refactoring?} This question explores how accurately the different models predict refactoring opportunities using Logistic Regression as a baseline.
    \item[RQ2] \textit{What are the Important Features in the Refactoring Prediction Models?} This question regards the features most relevant to the models and has the biggest impact on the outcome.
    \item[RQ3] \textit{How robust are the Predictive Models, and can they be used in different contexts?} Whether or not refactoring prediction models \textit{need} to be trained for this specific context or a more generalized model is sufficient potentially reduces the cost of applying and re-training the models in the real world. The question is tackled by comparing the accuracy of predictive models against independent datasets.
\end{itemize}
\subsubsection{Data Sources}
The targeted projects for the experiment are collected from three different sources: The Apache Software Foundation (ASF), F-Droid (a software repository of Android mobile apps), and GitHub. They also used a highly sophisticated method to extract the labelled instances using RefactoringMiner\cite{tsantalis2018accurate}, a tool for refactoring detection having an average precision of 99.6 percent\cite{tsantalis2022refactoringminer}.
\subsubsection{Answering the Research questions}
The evaluation gave answers to the three research questions formulated above.
\begin{itemize}
    \item[RQ1] Observation 1: \textit{Random Forest models demonstrate the highest accuracy in predicting software refactoring.} Their average accuracies for class-level, method-level, and variable-level refactorings are 0.93, 0.90, and 0.94, respectively.\\Observation 2: \textit{Neural Networks occasionally outperform Random Forest models.} Neural Networks surpassed Random Forest models in accuracy on four occasions, with a difference of around 1 per cent.\\ Observation 3: \textit{Naive Bayes models exhibit high recall with the cost of low precision.} These models achieved recalls of 0.94, 0.93, 0.94, and 0.84 on the combined dataset but had the lowest precision values: 0.62, 0.66, 0.62, and 0.67.\\ Observation 4: \textit{Logistic Regression maintains good accuracy.} It consistently outperforms Naive Bayes models, with an average accuracy of 0.83 across all datasets.
    \item[RQ2] Observation 5: \textit{Class-level refactorings highly depend on process metrics} Key metrics include the number of commits, the number of previous refactorings, and lines added in a commit. Process metrics frequently appear in the top rankings, with ownership metrics occasionally appearing.\\ Observation 6: \textit{Class-level features significantly impact method-level and variable-level refactorings.} For method-level refactoring models, 13 out of the top 17 features are class-level features. Similarly, for variable-level refactorings, 11 of the top features are class-level.\\ Observation 7: \textit{Certain features never appear in the rankings.}
    \item[RQ3] Observation 8: \textit{Random Forest models maintain strong precision and recall when generalized, though slightly lower than specific datasets.} Training Random Forest models with the GitHub dataset and testing it in Apache achieves a precision of 0.87 and recall of 0.84 and still performs reasonably well when trained on smaller datasets. However, Random Forest performs remarkably better when trained on the specific datasets.\\Observation 9: \textit{Models for method-level and variable-level refactorings perform worse than those for class-level refactorings.} Using Random Forest models trained on the GitHub dataset and tested on the F-Droid dataset, the average precision and recall for class-level refactorings are 0.92. In contrast, for method-level refactorings, the average precision and recall are 0.77 and 0.72, respectively, and for variable-level refactorings, they are 0.81 and 0.75.\\ Observation 10: \textit{SVM outperforms Decision Trees in generalization.}\\ Observation 11: \textit{Logistic Regression remains a reliable baseline.} When trained on the GitHub dataset and tested on the Apache dataset, it achieves an average precision and recall of 0.84 and 0.83. It performs the worst when tested on F-Droid and trained on the Apache dataset.\\ Observation 12: \textit{Heterogeneous datasets may generalize more effectively.} The Apache and F-Droid datasets show lower precision and recall in different contexts. Cross-testing these datasets does not exceed precision and recall values of 0.78, unlike the GitHub dataset, which is more heterogeneous.
\end{itemize}
\subsubsection{Summary}
The primary results of the experiment indicate that Random Forest models excel over other Machine Learning models in predicting software refactoring across most tests. Additionally, process and ownership metrics are vital in developing more effective models. Moreover, models trained on data from diverse projects generalize better and maintain strong performance.\\
Most importantly, Machine Learning algorithms can effectively and accurately model the refactoring recommendation problem.

\subsection{Summary}
Both of the covered experiments show great success with automating code refactoring. Out of the GPT models, GPT-4 outperformed the GPT-3.5 and GPT-3.5-Turbo variants. Out of the compared machine learning models, Random Forest models consistently outperform all other models in every field. Classic machine learning models should be trained on the dataset or at least homogeneous datasets as they perform best when trained and tested on the same datasets whenever possible. GPT-Models do not have such a restriction but may overall perform slightly worse than Random Forest Models.\\
The next step in this research field should be to compare GPT -4 with Random Forest strictly.